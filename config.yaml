defaults:
  processing_mode: one-to-one
  backend_type: llm
  inference: local
  export_format: csv
docling:
  pipeline: ocr
models:
  vlm:
    local:
      default_model: numind/NuExtract-2.0-8B
      provider: docling
  llm:
    local:
      default_model: ibm-granite/granite-4.0-1b
      provider: vllm
    remote:
      default_model: mistral-medium-latest
      provider: mistral
    providers:
      ollama:
        default_model: llama3:8b-instruct
      vllm:
        default_model: llama-3.1-8b
      mistral:
        default_model: mistral-medium-latest
      openai:
        default_model: gpt-4-turbo
      gemini:
        default_model: gemini-2.5-flash
output:
  default_directory: outputs
  create_visualizations: true
  create_markdown: true
