# Docling-Graph Configuration File
# This file defines default settings and model configurations

# Default settings (can be overridden via CLI flags)
defaults:
  processing_mode: many-to-one  # one-to-one | many-to-one
  model_type: llm               # llm | vlm
  inference: local              # local | api
  export_format: csv            # csv | cypher

# Model configurations organized by type and inference location
models:
  # Vision-Language Models (VLM)
  vlm:
    local:
      default_model: "numind/NuExtract-2.0-8B"
      provider: "docling"
      # Alternative models:
      # - "numind/NuExtract-2.0-2B" (faster, less accurate)
  
  # Language Models (LLM)
  llm:
    local:
      default_model: "llama3:8b-instruct"
      provider: "ollama"
      # Make sure Ollama is running locally with: ollama serve
      # Pull models with: ollama pull llama3:8b-instruct
      
    api:
      default_model: "mistral-small-latest"
      provider: "mistral"
      
      # Multiple API providers supported
      # Set API key: <PROVIDER>_API_KEY="your_key" using .env or export
      providers:
        mistral:
          default_model: "mistral-small-latest"
          
        openai:
          default_model: "gpt-4-turbo"
          
        anthropic:
          default_model: "claude-3-opus-20240229"

# Output settings
output:
  default_directory: "outputs"
  create_visualizations: true
  create_markdown: true
